{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nifH combined analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to take the union of unique results of round one and round two of the gene-finder, in which sequences from the Tara metagenomes were extracted using reference alignments for a specific gene of interest (here nifH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main paths to facilitate combining the results of rounds 1 and 2 snakemake automated gene-finder searches \n",
    "gene1='nifH'\n",
    "gene2='nifH_rnd2'\n",
    "outputs1= '/vortexfs1/omics/alexander/lblum/tara_gene_finder/jupyter_notebooks/outputs/nifH'\n",
    "outputs2='/vortexfs1/omics/alexander/lblum/tara_gene_finder/jupyter_notebooks/outputs/nifH_rnd2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locations of the tables from rounds 1 and 2 detailing sequences meeting threshold e-values.\n",
    "threshold_1_path=os.path.join(outputs1,gene1+\"_threshold_hits_table.csv\")\n",
    "threshold_2_path=os.path.join(outputs2,gene2+\"_threshold_hits_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputing our sample info file which contains all the assigned size fractions\n",
    "sample_info=pd.read_csv('/vortexfs1/omics/alexander/lblum/tara_gene_finder/data/metadata/SampleList_ForAssembly_metaG_python.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using bash magic here to combine our fasta hits from both rounds and then another one liner to remove duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat outputs/nifH/nifH_extracted_hits.fasta outputs/nifH_rnd2/nifH_rnd2_extracted_hits.fasta > outputs/nifH/nifH_combined_gene_hits.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "awk '/^>/{f=!d[$1];d[$1]=1}f' outputs/nifH/nifH_combined_gene_hits.fasta > outputs/nifH/nifH_combined_gene_hits_dedup.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info[\"Depth\"]=sample_info.Depth_sizefrac.str.split(\"-\").str[0]\n",
    "#here we create a new column called depth which took threshold_with_info column Depth_sizefrac, and split it based on the dash, then called the first half\n",
    "sample_info[\"Sizefrac\"]=sample_info.Depth_sizefrac.str.split(\"-\",1).str[1]\n",
    "#new column with sizefrac, gave it the qualifier to only split it 1 time so that it retained the sizefrac range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our two threshold hits tables from csv in the appropriate output folders\n",
    "threshold_1=pd.read_csv(threshold_1_path)\n",
    "threshold_2=pd.read_csv(threshold_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union of these two threshold tables, with duplicates searched based on contig_id\n",
    "threshold_union= pd.concat([threshold_1, threshold_2]).drop_duplicates(subset= 'contig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#threshold_union will go to a csv in our outputs folder for the main gene directory with \"combo\" in the identifier\n",
    "threshold_union.to_csv(os.path.join(outputs1,gene1+\"_combo_threshold_hits_table.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_threshold=threshold_union.groupby('sample_id').count()\n",
    "#use this to normalize our data by ERR_count\n",
    "#merge normalized_threshold which contains coutns of sample_ids with our new sample_info table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_threshold_info= normalized_threshold.merge(sample_info, left_on='sample_id', right_on='Assembly_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column called 'normalized_count' which we can use for each graph of interest (won't change across groupings)\n",
    "normalized_threshold_info['normalized_count']=normalized_threshold_info.contig_id/normalized_threshold_info.ERR_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
